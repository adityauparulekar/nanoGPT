{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out-rare_history' # ignored if init_from is not 'resume'\n",
    "bias = 0.5\n",
    "num_samples = 1000 # number of samples to draw\n",
    "max_new_tokens = 100 # number of tokens generated in each sample\n",
    "temperature = 1 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile_bool = False # use PyTorch 2.0 to compile the model to be faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out-rare_history 0.5\n"
     ]
    }
   ],
   "source": [
    "print(out_dir, bias)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.84M\n",
      "number of parameters: 841536.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(3, 108)\n",
       "    (wpe): Embedding(3, 108)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=108, out_features=324, bias=False)\n",
       "          (c_proj): Linear(in_features=108, out_features=108, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=108, out_features=432, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=432, out_features=108, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=108, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "print(load_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta from data/rare_history/meta.pkl...\n",
      "{'\\n': 0, '0': 1, '1': 2} {0: '\\n', 1: '0', 2: '1'}\n",
      "{'0': 0.25, '1': 0.75, '00': 0.5, '01': 0.5, '10': 0.5, '11': 0.75, '000': 0.25, '001': 0.75, '010': 0.25, '011': 0.25, '100': 0.5, '101': 0.75, '110': 0.75, '111': 0.75, '0000': 0.75, '0001': 0.75, '0010': 0.5, '0011': 0.25, '0100': 0.75, '0101': 0.5, '0110': 0.5, '0111': 0.5, '1000': 0.25, '1001': 0.75, '1010': 0.75, '1011': 0.5, '1100': 0.5, '1101': 0.75, '1110': 0.5, '1111': 0.5, '00000': 0.5, '00001': 0.25, '00010': 0.25, '00011': 0.5, '00100': 0.75, '00101': 0.25, '00110': 0.75, '00111': 0.75, '01000': 0.25, '01001': 0.5, '01010': 0.25, '01011': 0.75, '01100': 0.75, '01101': 0.5, '01110': 0.25, '01111': 0.75, '10000': 0.5, '10001': 0.75, '10010': 0.75, '10011': 0.75, '10100': 0.75, '10101': 0.25, '10110': 0.5, '10111': 0.5, '11000': 0.5, '11001': 0.25, '11010': 0.5, '11011': 0.25, '11100': 0.75, '11101': 0.75, '11110': 0.75, '11111': 0.75, '': 0.5}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading meta from {meta_path}...\")\n",
    "with open(meta_path, 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "# TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "stoi, itos = meta['stoi'], meta['itos']\n",
    "print(stoi, itos)\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "markov = {}\n",
    "size = 5\n",
    "rng = np.random.default_rng(42)\n",
    "for s in range(1, size+1):\n",
    "    # print(s)\n",
    "    for i in range(2**s):\n",
    "        curr_seed = format(i, f'0{s}b')\n",
    "        # print(i, curr_seed)\n",
    "        bias = rng.choice([0.25, 0.5, 0.75])\n",
    "        markov[curr_seed] = bias\n",
    "markov[''] = 0.5\n",
    "print(markov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"\\n\"\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "1010101101101100100100001101011011011001010110100000010101101100111001011011000011000001010011100101\n",
      "0.50732374 0.5\n",
      "0.75527614 0.75\n",
      "0.50585914 0.5\n",
      "0.5058591 0.75\n",
      "0.5053709 0.75\n",
      "0.5058591 0.25\n",
      "0.5053709 0.25\n",
      "0.5058591 0.25\n",
      "0.755637 0.75\n",
      "0.50488263 0.5\n",
      "0.5058591 0.5\n",
      "0.755637 0.25\n",
      "0.50488263 0.5\n",
      "0.5058591 0.5\n",
      "0.755637 0.25\n",
      "0.50488263 0.5\n",
      "0.5068355 0.75\n",
      "0.50634736 0.25\n",
      "0.5053709 0.75\n",
      "0.5068355 0.75\n",
      "0.50634736 0.5\n",
      "0.5053709 0.75\n",
      "0.5068355 0.75\n",
      "0.5073237 0.25\n",
      "0.5073237 0.5\n",
      "0.50634736 0.25\n",
      "0.755637 0.5\n",
      "0.50488263 0.75\n",
      "0.5058591 0.5\n",
      "0.5053709 0.5\n",
      "0.5058591 0.25\n",
      "0.755637 0.75\n",
      "0.50488263 0.5\n",
      "0.5058591 0.5\n",
      "0.755637 0.25\n",
      "0.50488263 0.5\n",
      "0.5058591 0.5\n",
      "0.755637 0.25\n",
      "0.50488263 0.5\n",
      "0.5068355 0.75\n",
      "0.50634736 0.25\n",
      "0.5053709 0.75\n",
      "0.5058591 0.25\n",
      "0.5053709 0.25\n",
      "0.5058591 0.25\n",
      "0.755637 0.75\n",
      "0.50488263 0.5\n",
      "0.5058591 0.5\n",
      "0.5053709 0.5\n",
      "0.5068355 0.75\n",
      "0.5073237 0.25\n",
      "0.5073237 0.5\n",
      "0.5073237 0.5\n",
      "0.5073237 0.5\n",
      "0.50634736 0.25\n",
      "0.5053709 0.25\n",
      "0.5058591 0.25\n",
      "0.5053709 0.25\n",
      "0.5058591 0.25\n",
      "0.755637 0.75\n",
      "0.50488263 0.5\n",
      "0.5058591 0.5\n",
      "0.755637 0.25\n",
      "0.50488263 0.5\n",
      "0.5068355 0.75\n",
      "0.50634736 0.25\n",
      "0.755637 0.75\n",
      "0.75527614 0.75\n",
      "0.50488263 0.25\n",
      "0.5068355 0.75\n",
      "0.50634736 0.25\n",
      "0.5053709 0.75\n",
      "0.5058591 0.25\n",
      "0.755637 0.75\n",
      "0.50488263 0.5\n",
      "0.5058591 0.5\n",
      "0.755637 0.25\n",
      "0.50488263 0.5\n",
      "0.5068355 0.75\n",
      "0.5073237 0.5\n",
      "0.5073237 0.5\n",
      "0.50634736 0.25\n",
      "0.755637 0.5\n",
      "0.50488263 0.75\n",
      "0.5068355 0.75\n",
      "0.5073237 0.5\n",
      "0.5073237 0.5\n",
      "0.5073237 0.5\n",
      "0.50634736 0.25\n",
      "0.5053709 0.25\n",
      "0.5058591 0.25\n",
      "0.5053709 0.25\n",
      "0.5068355 0.75\n",
      "0.50634736 0.5\n",
      "0.755637 0.75\n",
      "0.75527614 0.75\n",
      "0.50488263 0.25\n",
      "0.5068355 0.75\n",
      "0.50634736 0.25\n",
      "0.5053709 0.75\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# true_prob_array = []\n",
    "# model_prob_array = []\n",
    "prob_diff_array = []\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(1):\n",
    "            print(k)\n",
    "            while True:\n",
    "                y = model.generate_probs(x, max_new_tokens, temperature=temperature, top_k=None)\n",
    "                final_out = decode(y[0][0].tolist())\n",
    "                print(final_out)\n",
    "                invalid = False\n",
    "                for i in range(1, len(final_out)):\n",
    "                    c = final_out[i]\n",
    "                    if c not in '01':\n",
    "                        print(f\"{c}, {i}\")\n",
    "                        invalid = True\n",
    "                        break\n",
    "                if invalid:\n",
    "                    continue\n",
    "                sample = y[0]\n",
    "                probs = y[1]\n",
    "                # true_probs = []\n",
    "                # model_probs = []\n",
    "                prob_diffs = []\n",
    "                for i in range(len(probs)):\n",
    "                    context = final_out[1:i+1][-size:]\n",
    "                    zero_p = probs[i][1] / (probs[i][1] + probs[i][2])\n",
    "                    # true_probs.append(markov[context])\n",
    "                    # model_probs.append(zero_p)\n",
    "                    print(zero_p, markov[context])\n",
    "                    prob_diffs.append(zero_p - markov[context])\n",
    "                i = np.argmax(np.abs(prob_diffs))\n",
    "                break\n",
    "                # print(final_out)\n",
    "                # print('---------------')\n",
    "            # true_prob_array.append(true_probs)\n",
    "            # model_prob_array.append(model_probs)\n",
    "            prob_diff_array.append(prob_diffs)\n",
    "# true_prob_array = np.array(true_prob_array)\n",
    "# model_prob_array = np.array(model_prob_array)\n",
    "prob_diff_array = np.array(prob_diff_array)\n",
    "print(np.argmax(prob_diff_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5065558552742004 0.25\n",
      "1 0.5182334184646606 0.75\n",
      "00 0.24503326416015625 0.5\n",
      "01 0.7552562355995178 0.5\n",
      "10 0.2450280636548996 0.5\n",
      "11 0.7550386786460876 0.75\n",
      "000 0.5074297785758972 0.25\n",
      "001 0.5066708326339722 0.75\n",
      "010 0.5055210590362549 0.25\n",
      "011 0.7554191946983337 0.25\n",
      "100 0.5072233080863953 0.5\n",
      "101 0.5060727596282959 0.75\n",
      "110 0.5054183602333069 0.75\n",
      "111 0.7552872896194458 0.75\n"
     ]
    }
   ],
   "source": [
    "markov = {'0': 0.25, '1': 0.75, '00': 0.5, '01': 0.5, '10': 0.5, '11': 0.75, '000': 0.25, '001': 0.75, '010': 0.25, '011': 0.25, '100': 0.5, '101': 0.75, '110': 0.75, '111': 0.75}\n",
    "k = markov.keys()\n",
    "for c in k:\n",
    "    start_ids = encode(c)\n",
    "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "    logits, _ = model(x)\n",
    "    logits = logits[:, -1, :]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    print(c, (probs[0][1]/(probs[0][1] + probs[0][2])).item(), markov[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oCCURENCES OF 000: 601587\n",
      "PERCENT OF 0 after: 0.20098389554308985\n",
      "oCCURENCES OF 001: 1176250\n",
      "PERCENT OF 0 after: 0.7330084768665734\n",
      "oCCURENCES OF 010: 1507572\n",
      "PERCENT OF 0 after: 0.3217461757528418\n",
      "oCCURENCES OF 011: 792536\n",
      "PERCENT OF 0 after: 0.24727664524153498\n",
      "oCCURENCES OF 100: 1183421\n",
      "PERCENT OF 0 after: 0.5221423462630254\n",
      "oCCURENCES OF 101: 1256676\n",
      "PERCENT OF 0 after: 0.6623683449495549\n",
      "oCCURENCES OF 110: 788965\n",
      "PERCENT OF 0 after: 0.7530659173844361\n",
      "oCCURENCES OF 111: 600402\n",
      "PERCENT OF 0 after: 0.7986375130231685\n"
     ]
    }
   ],
   "source": [
    "f = open('data/rare_history/data.txt')\n",
    "s = ''.join(f.readlines())\n",
    "for c in k:\n",
    "    if len(c) == 3:\n",
    "        print(f\"oCCURENCES OF {c}: {s.count(c)}\")\n",
    "        print(f\"PERCENT OF 0 after: {s.count(c + '0') / (s.count(c + '0') + s.count(c + '1'))}\")\n",
    "        print(markov[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
